{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "\n",
    "def download_glove_embeddings(url, filename):\n",
    "        print(f\"Downloading GloVe embeddings from {url}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded GloVe embeddings to {filename}\")\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    if not os.path.isfile(glove_file_path):\n",
    "        # If the file does not exist, download it\n",
    "        url = \"http://nlp.stanford.edu/data/glove.6B.zip\"  # URL for GloVe 6B 300d\n",
    "        zip_file_path = \"glove.6B.zip\"\n",
    "        if not os.path.isfile(zip_file_path):\n",
    "            download_glove_embeddings(url, zip_file_path)\n",
    "        # Unzip the file\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        if not os.path.isfile(glove_file_path):\n",
    "            raise FileNotFoundError(f\"Expected file {glove_file_path} not found after extraction.\")\n",
    "\n",
    "load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import load_data\n",
    "\n",
    "df = load_data(\"../data/traindata.csv\")\n",
    "devdf = load_data(\"../data/devdata.csv\")\n",
    "\n",
    "print(len(df))\n",
    "print(len(devdf))\n",
    "\n",
    "print(df.head())\n",
    "print(devdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"aspect_category\"].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"polarity\"].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import remove_stopwords, load_data, simple_tokenize\n",
    "from stopwords import STOPWORDS\n",
    "import random\n",
    "\n",
    "df = load_data(\"../data/traindata.csv\")\n",
    "\n",
    "rand_i = random.randint(0, len(df)-1)\n",
    "# rand_i = 0\n",
    "# rand_i = 303\n",
    "# rand_i = 1111\n",
    "\n",
    "print(\"index\", rand_i)\n",
    "\n",
    "print(df[\"aspect_category\"][rand_i])\n",
    "target_tok = simple_tokenize(df[\"aspect_category\"][rand_i].lower())\n",
    "print(len(target_tok), target_tok)\n",
    "\n",
    "print(df[\"aspect_term\"][rand_i])\n",
    "target_tok = remove_stopwords(simple_tokenize(df[\"aspect_term\"][rand_i].lower()), STOPWORDS)\n",
    "print(len(target_tok), target_tok)\n",
    "\n",
    "print(df[\"sentence\"][rand_i])\n",
    "sentence_tok = remove_stopwords(simple_tokenize(df[\"sentence\"][rand_i].lower()), STOPWORDS)\n",
    "print(len(sentence_tok), sentence_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import load_glove_embeddings\n",
    "\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "word_to_idx, embedding_matrix = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import remove_stopwords, load_data, simple_tokenize, pad_sequence\n",
    "import random\n",
    "\n",
    "max_len = 50\n",
    "STOPWORDS = []\n",
    "\n",
    "df = load_data(\"../data/traindata.csv\")\n",
    "column = \"aspect_term\"\n",
    "\n",
    "rand_i = random.randint(0, len(df)-1)\n",
    "# rand_i = 0\n",
    "# rand_i = 303\n",
    "# rand_i = 1111\n",
    "\n",
    "tokens = remove_stopwords(simple_tokenize(df[column][rand_i].lower()), STOPWORDS)\n",
    "indices = [word_to_idx.get(token, word_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "indices = pad_sequence(indices, word_to_idx, max_len) \n",
    "\n",
    "print(\"index\", rand_i)\n",
    "print(df[column][rand_i])\n",
    "print(tokens)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Left and Right sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import remove_stopwords, load_data, simple_tokenize\n",
    "from stopwords import STOPWORDS\n",
    "import random\n",
    "\n",
    "df = load_data(\"../data/traindata.csv\")\n",
    "\n",
    "rand_i = random.randint(0, len(df)-1)\n",
    "# rand_i = 0\n",
    "# rand_i = 303\n",
    "# rand_i = 1111\n",
    "\n",
    "print(\"index\", rand_i)\n",
    "\n",
    "# start_index = df[offset]\n",
    "# print()\n",
    "\n",
    "start_index, end_index = df[\"offset\"][rand_i].split(\":\")\n",
    "start_index, end_index = int(start_index), int(end_index)\n",
    "\n",
    "sentence = df[\"sentence\"][rand_i]\n",
    "left_sentence = sentence[:end_index]\n",
    "right_sentence = sentence[start_index:]\n",
    "\n",
    "print(df[\"aspect_term\"][rand_i])\n",
    "print(sentence)\n",
    "print(left_sentence)\n",
    "print(right_sentence)\n",
    "\n",
    "print(simple_tokenize(sentence))\n",
    "print(simple_tokenize(left_sentence))\n",
    "print(simple_tokenize(right_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import remove_stopwords, load_data, simple_tokenize\n",
    "from stopwords import STOPWORDS\n",
    "# STOPWORDS = []\n",
    "\n",
    "# filepath = \"../data/traindata.csv\"\n",
    "filepath = \"../data/devdata.csv\"\n",
    "\n",
    "column = \"sentence\"\n",
    "\n",
    "df = load_data(filepath)\n",
    "max_tokens = []\n",
    "max_len = 0\n",
    "max_i = -1\n",
    "\n",
    "for i, elem in enumerate(df[column]):\n",
    "    tokens = remove_stopwords(simple_tokenize(elem.lower()), STOPWORDS)\n",
    "    if len(tokens) > max_len:\n",
    "        max_tokens = tokens\n",
    "        max_len = len(tokens)\n",
    "        max_i = i\n",
    "\n",
    "print(df[column][max_i])\n",
    "print(max_tokens)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(set(df[\"aspect_term\"]))\n",
    "longest_str = max(l, key=len)\n",
    "index_of_longest = list(df[\"aspect_term\"]).index(longest_str)\n",
    "\n",
    "print(len(longest_str))\n",
    "print(longest_str)\n",
    "print(\"index\", index_of_longest)\n",
    "print(sum(len(s) > 50 for s in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(set(df[\"sentence\"]))\n",
    "longest_str = max(l, key=len)\n",
    "index_of_longest = list(df[\"sentence\"]).index(longest_str)\n",
    "\n",
    "print(len(longest_str))\n",
    "print(longest_str)\n",
    "print(\"index\", index_of_longest)\n",
    "print(sum(len(s) > 50 for s in l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM Attention Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bilstm_attention import BiLSTM_Attention_Dataset\n",
    "from preprocess import load_glove_embeddings\n",
    "\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "word_to_idx, embedding_matrix = load_glove_embeddings(glove_file_path)\n",
    "max_len = 50\n",
    "\n",
    "train_dataset = BiLSTM_Attention_Dataset(\"../data/traindata.csv\", word_to_idx, max_len)\n",
    "dev_dataset = BiLSTM_Attention_Dataset(\"../data/devdata.csv\", word_to_idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_i = random.randint(0, len(train_dataset)-1)\n",
    "# rand_i = 0\n",
    "\n",
    "train_dataset[rand_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=64)\n",
    "\n",
    "for i, batch in enumerate(train_data_loader):\n",
    "    print(batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def __getitem__(self, index):\n",
    "        \n",
    "    #     # Get the index of the aspect term in the sentence\n",
    "    #     aspect_term_indices_in_sentence = [i for i, token in enumerate(sentence_tokens) if token in aspect_term_tokens]\n",
    "    #     if not aspect_term_indices_in_sentence:\n",
    "    #         aspect_term_indices_in_sentence = [len(sentence_tokens) // 2]\n",
    "        \n",
    "    #     aspect_term_position = aspect_term_indices_in_sentence[0]\n",
    "\n",
    "    #     # Determine the start and end indices of the context window\n",
    "    #     start_index = max(0, aspect_term_position - self.context_window)\n",
    "    #     end_index = min(len(sentence_tokens), aspect_term_position + self.context_window + 1)\n",
    "\n",
    "    #     # Extract the context window\n",
    "    #     context_window_tokens = sentence_tokens[start_index:end_index]\n",
    "    #     context_window_indices = [self.word_to_idx.get(token, self.word_to_idx[\"<UNK>\"]) for token in context_window_tokens]\n",
    "\n",
    "    #     # Pad or truncate the context window to the max_len\n",
    "    #     context_window_indices = self.pad_sequence(context_window_indices, self.max_len)\n",
    "\n",
    "    #     # Pad or truncate the aspect term indices\n",
    "    #     aspect_term_indices = self.pad_sequence(aspect_term_indices, self.max_len)\n",
    "\n",
    "    #     return {\n",
    "    #         \"aspect_term_indices\": torch.tensor(aspect_term_indices, dtype=torch.long),\n",
    "    #         \"sentence_indices\": torch.tensor(context_window_indices, dtype=torch.long),\n",
    "    #         \"labels\": torch.tensor(polarity, dtype=torch.long),\n",
    "    #     }\n",
    "\n",
    "    # def simple_tokenize(self, text):\n",
    "    #     return re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "    # def pad_sequence(self, seq, max_len):\n",
    "    #     if len(seq) < max_len:\n",
    "    #         seq += [self.word_to_idx[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    #     else:\n",
    "    #         seq = seq[:max_len]\n",
    "    #     return seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD LSTM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdlstm import TD_LSTM_Dataset\n",
    "from preprocess import load_data, load_glove_embeddings\n",
    "\n",
    "dataset = TD_LSTM_Dataset\n",
    "\n",
    "train_filename = \"../data/traindata.csv\"\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "word_to_idx, embedding_matrix = load_glove_embeddings(glove_file_path)\n",
    "max_len = 50\n",
    "\n",
    "df = load_data(train_filename)\n",
    "dataset = dataset(train_filename, word_to_idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(dataset, TD_LSTM_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_i = random.randint(0, len(dataset)-1)\n",
    "\n",
    "print(df[\"aspect_term\"][rand_i])\n",
    "print(df[\"sentence\"][rand_i])\n",
    "dataset[rand_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATAE LSTM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atae_lstm import ATAE_LSTM_Dataset\n",
    "from preprocess import load_data, load_glove_embeddings\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = ATAE_LSTM_Dataset\n",
    "\n",
    "train_filename = \"../data/traindata.csv\"\n",
    "dev_filename = \"../data/devdata.csv\"\n",
    "\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "word_to_idx, embedding_matrix = load_glove_embeddings(glove_file_path)\n",
    "max_len = 50\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "df = load_data(train_filename)\n",
    "\n",
    "train_dataset = dataset(train_filename, word_to_idx, max_len)\n",
    "dev_dataset = dataset(dev_filename, word_to_idx, max_len)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_i = random.randint(0, len(dataset)-1)\n",
    "\n",
    "print(df[\"aspect_term\"][rand_i])\n",
    "print(df[\"sentence\"][rand_i])\n",
    "dataset[rand_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "aspect_indices = dataset[rand_i][\"aspect_indices\"]\n",
    "sentence_indices = dataset[rand_i][\"sentence_indices\"]\n",
    "pad_index = 0\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "sentence_embeddings = embedding(sentence_indices).unsqueeze(0)\n",
    "sentence_embeddings.size()\n",
    "sentence_embeddings\n",
    "\n",
    "aspect_embeddings = embedding(aspect_indices).unsqueeze(0)\n",
    "aspect_embeddings.size()\n",
    "aspect_embeddings\n",
    "\n",
    "pad_mask = (aspect_indices != pad_index).float().unsqueeze(0)\n",
    "pad_mask.size() # (batch_size, aspect_len)\n",
    "pad_mask\n",
    "\n",
    "masked_aspect_embeddings = aspect_embeddings * pad_mask.unsqueeze(-1)\n",
    "masked_aspect_embeddings.size()\n",
    "masked_aspect_embeddings\n",
    "\n",
    "sum_aspect_embeddings = masked_aspect_embeddings.sum(dim=1)\n",
    "sum_aspect_embeddings.size()\n",
    "sum_aspect_embeddings\n",
    "\n",
    "num_non_padding_tokens = pad_mask.sum(dim=1)\n",
    "num_non_padding_tokens.size()\n",
    "num_non_padding_tokens\n",
    "\n",
    "mean_aspect_embeddings = sum_aspect_embeddings / num_non_padding_tokens.unsqueeze(-1)\n",
    "mean_aspect_embeddings.size()\n",
    "mean_aspect_embeddings\n",
    "\n",
    "repeated_aspect_embeddings = mean_aspect_embeddings.unsqueeze(1).repeat(\n",
    "    1, sentence_embeddings.size(1), 1\n",
    ")\n",
    "repeated_aspect_embeddings.size()\n",
    "repeated_aspect_embeddings\n",
    "\n",
    "x = torch.cat((sentence_embeddings, repeated_aspect_embeddings), 2)\n",
    "x.size()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from preprocess import load_glove_embeddings\n",
    "\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "word_to_idx, embedding_matrix = load_glove_embeddings(glove_file_path)\n",
    "max_len = 50\n",
    "\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1, 32, 128).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATAE LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atae_lstm import ATAE_LSTM_Dataset, ATAE_LSTM_Model\n",
    "from preprocess import load_data, load_glove_embeddings\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = ATAE_LSTM_Dataset\n",
    "\n",
    "train_filename = \"../data/traindata.csv\"\n",
    "dev_filename = \"../data/devdata.csv\"\n",
    "\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "word_to_idx, embedding_matrix = load_glove_embeddings(glove_file_path)\n",
    "max_len = 50\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "df = load_data(train_filename)\n",
    "\n",
    "train_dataset = dataset(train_filename, word_to_idx, max_len)\n",
    "dev_dataset = dataset(dev_filename, word_to_idx, max_len)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = ATAE_LSTM_Model(\n",
    "    embedding_matrix, \n",
    "    128,\n",
    "    3, \n",
    "    False,\n",
    "    1,\n",
    "    0.7, \n",
    "    50)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def get_batch(data_loader):\n",
    "    for batch in data_loader:\n",
    "        return batch\n",
    "\n",
    "batch = get_batch(train_data_loader)\n",
    "\n",
    "pad_index = word_to_idx[\"<PAD>\"]\n",
    "\n",
    "sentence_indices = batch[\"sentence_indices\"].to(device)\n",
    "aspect_indices = batch[\"aspect_indices\"].to(device)\n",
    "labels = batch[\"labels\"].to(device)\n",
    "\n",
    "bs = sentence_indices.size(0)\n",
    "\n",
    "h0, c0 = model.init_prev_hidden(bs)\n",
    "h0 = h0.to(device)\n",
    "c0 = c0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, H = model(sentence_indices, aspect_indices, pad_index, (h0, c0))\n",
    "print(output.size())\n",
    "print(output)\n",
    "print(labels)\n",
    "print(H.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print(input)\n",
    "print(target)\n",
    "print(output)\n",
    "\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(input)\n",
    "print(target)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
